# imports
import numpy as np
import matplotlib.pyplot as plt
import random
from collections import defaultdict
from pathlib import Path
import pickle

class supercross_env: 
  # Define constants for environment model
  def __init__(self,trk,height=0.0,endTime=30.0,sK=11e3,sB=2700.0,sSag=0.3,bkXstart=0.0,sKnonLin_flg=True,stateTrkResolution=1,stateTrkVisonDist=50,drawRace_flg=False,draw_race_frequency=500,save_dir_str=None):
    
    #UI controls
    self.drawRace_flg = drawRace_flg
    self.episode_cnt = 0 # a counter tracking the number of episodes completed for training, used to draw_race every once in a while
    self.draw_race_frequency = draw_race_frequency
    self.save_dir_str = save_dir_str
        
    # constants
    self.g = -9.81
    
    # track
    self.trkSet = trk
    
    # agent performance tracking
    self.raceTimes = defaultdict(list)
    self.raceTimesEp = defaultdict(list)
    self.bestTimes = defaultdict(list)
    self.bestTimesEp = defaultdict(list)
    
    # agent interface attributes
    # states
    self.stateTrkResolution = stateTrkResolution #distance increment between samples for agents view of track ahead
    self.stateTrkVisonDist = stateTrkVisonDist #how far in disatnce the agent sees ahead
    self.stateTrkNumel = int(np.floor(self.stateTrkVisonDist/self.stateTrkResolution))
    self.stateShape = int(self.stateTrkNumel + 3)
    self._states = dict(shape=(self.stateShape,), type='float')
    # actions
    self._actions = dict(type='float', shape=(), min_value=-1, max_value=1)
    self.tensorForceExecuteN = 100
    # rewards
    self.rewards_aveSpd4WorstTime = 10/3.6 # average speed of 10kph is defined as the wrost amount of time it could take to complete a track
    self.t_end = {}
    self.rewards_bestTimes = {}
    for kk in self.trkSet.keys():
      # compute and set the time limit for the track
      tmp = self.trkSet.get(kk)
      self.t_end[kk] = tmp[0,-1]/self.rewards_aveSpd4WorstTime
      fnam = save_dir_str + 'bestRace_' + kk + '.pkl'
      best_file = Path(fnam)
      if best_file.is_file():
        pkl_file = open(fnam, 'rb') # connect to the pickled data
        trkEnv = pickle.load(pkl_file) # load it into a variable
        self.rewards_bestTimes[kk] = trkEnv.rewards_bestTimes[kk]
      else:
        self.rewards_bestTimes[kk] = self.t_end[kk]
    self.rewards_slowerThanBestTimeFactor = 2
    self.rewards_fasterThanBestTimeFactor = 1
    self.rewards_newBestTimeSet = False

    # define simulation time step
    self.dt = 0.001
    
    # wheel
    self.whlR = (19/2+2)*0.0254 # ~0.3meters
    self.whlM = 15
    self.whlJ = 0.25*10
    
    # bike
    self.bkM = (220 - self.whlM + 160) / 2.2
    self.bkPwr = 35 * 745.7 # 55HP->Watts
    self.bkTrq = 0.8*-self.g*self.bkM*self.whlR
    self.bkCrr = 0.015
    self.bkAero = 0.5*1.2*1*0.7 # 1/2*rho*A*Cd
    
    # suspension
    self.sK = sK # spring constant #https://www.dirtrider.com/features/understanding-your-suspension-sag#page-3, then using 105mm-35mm=70mm, and 170lb ridere => 10800N/m
    self.sB = sB # damping const. compression and rebound. critically damped for sK=11e3N/m, and m=166kg => 2700
    self.sFbSat=np.abs(self.bkM*-self.g*10) # damping force saturation
    self.sT = 0.3 # total travel
    self.sSag = sSag # intial sag as a fraction of sT. aSag=0 means means there is no sag at rest, sSag=1 means the suspension is fully compressed at rest. link used for sK says 33% is good target
    self.sPL = self.bkM*-self.g - self.sK*self.sT*self.sSag # spring preload=force gravity minus the force generated by the target amount of sag
    self.sKnonLin_flg = sKnonLin_flg
    
    # brakes
    self.brkSpdThr = 0.5; # mps, speed threshold below which, brake trq is ramped down to zero
    
    # tire grip model - nu*Fy
    self.nu = 2    
      
    self.heightInit = height
    self.bkXstart = bkXstart
    
    supercross_env.reset(self)
    
  def get_max_time_steps(self):
    max_steps = max(self.t_end.values())
    max_steps = max_steps/self.dt/self.tensorForceExecuteN
    return max_steps
  
  # draw results of race
  def draw_race(self,raceName=None,saveFig=None,ep=None):
    fig =plt.figure()
    ax306 = fig.add_subplot(211)
    ax306.plot(self.bkX[0:self.i],self.throttle[0:self.i], label='throttle')
    ax306.plot(self.bkX[0:self.i],self.bkY[0:self.i], label='bk')
    ax306.plot(self.trkX[0:self.i],self.trkY[0:self.i], label='trk')
    ax306.legend()
    ax306.grid()
    ax306.set_xlabel('X pos(m)')
    ax306.set_ylabel('Y pos(m)/throttle(norm)')
    if raceName is None:
      ax306.set_title('bike trajectory')
    else:
      ax306.set_title(raceName)
      
    ax307 = fig.add_subplot(212)
    ax307.plot(self.t[0:self.i],np.multiply(self.throttle[0:self.i],10), label='throttle(norm*10)')
    ax307.plot(self.t[0:self.i],self.bkX[0:self.i], label='xpos(m)')
    ax307.plot(self.t[0:self.i],self.bkv[0:self.i], label='spd(m/s)')
    ax307.legend()
    ax307.grid()
    ax307.set_xlabel('time (s)')
    if saveFig:
      fnam = self.save_dir_str + 'best_race_' + self.trkKey + '_ep' + str(ep) + '.png'
      fig.savefig(fnam)
      plt.close(fig)
  
  # Define function to creation state, since multiple tensorForce API functions need the method
  def mk_state(self):
    # 1] calc remaining track distance
    remDist = self.trkX[-1] - self.bkX[self.i]
    if remDist < 1:
      trkY_samples = np.multiply(np.ones(int(self.stateTrkNumel)),-1)
    else:
      # find trkX index that is stateTrkVisonDist meters ahead
      idx1 = (np.abs(self.trkX - self.bkX[self.i])).argmin()
      if remDist < self.stateTrkVisonDist:
        idx2 = -1 # pick the end
        trkX_samples = np.arange(self.bkX[self.i],self.trkX[-1],self.stateTrkResolution)
      else:
        idx2 = (np.abs(self.trkX - (self.bkX[self.i]+self.stateTrkVisonDist*1.05))).argmin()
        # 2] get trkX and trkY data points that are stateTrkVisonDist ahead
        # 3] resample to X direction resolution of stateTrkResolution meters
        trkX_samples = np.arange(self.bkX[self.i],(self.bkX[self.i]+self.stateTrkVisonDist*1.05),self.stateTrkResolution)
      trkY_samples = np.interp(trkX_samples,self.trkX[idx1:idx2],self.trkY[idx1:idx2])
    if trkY_samples.size < self.stateTrkNumel:
      trkY_samples = np.append(trkY_samples, np.multiply(np.ones(int(self.stateTrkNumel)),-1))
    # 4] ensure the number of points is equal to stateTrkNumel, crop additional points from furthest ahead
    trkY_samples = trkY_samples[0:int(self.stateTrkNumel)]
    # 5] get all other state variables
    whlElev = (self.whlY[self.i] - self.whlR) - trkY_samples[0]
    bkSpd = self.bkv[self.i]
    sT = self.sTY[self.i]
    # 6] concatenate into a single state vector
    state = np.append(trkY_samples, [whlElev, bkSpd, sT])
    return state
  # Define tensorForce API functions
  @property
  def actions(self):
    # np.arange(-1,1,0.1)
    return self._actions
  
  def close(self):
    self.done = True
    return
  
  def execute(self,action):
    self.step(action,self.tensorForceExecuteN) # 100 physics time steps between each agent time step.
    next_state = self.mk_state()
    return  next_state, self.done, self.reward
  
  def reset(self):
    # set track for next episode
    self.trkKey,self.trk = random.choice(list(self.trkSet.items()))

    # create time vector          
    self.t=np.arange(0,self.t_end[self.trkKey],self.dt)
    
    # increment episode counter
    self.episode_cnt += 1
    #check if it's time to draw the race
#    if np.mod(self.episode_cnt,self.draw_race_frequency) == 0:
#      draw_race(self) 
    # reset best time tracker
    self.rewards_newBestTimeSet = False
    
    # defined track
    self.trkStep = 0.05
    self.trkX = np.arange(self.trk[0,0],self.trk[0,-1],0.05)
    self.trkY = np.interp(self.trkX,self.trk[0,:],self.trk[1,:])
    self.trkTheta = np.arctan(np.interp(self.trkX,self.trk[0,:],self.trk[2,:]))

    # some track data for other computation outside the env
#    self.trkYmax = np.max(self.trkY)
#    self.trkXSampled = self.trkX[0::10]
#    self.trkYSampled = self.trkY[0::10]
    
    # defined initial conditions
    self.bkX1 = self.trkX[0] + self.bkXstart
    
    self.whlY1 = self.trkY[0]+self.whlR + self.heightInit # "+ height" allows user to lift the bike off the ground for a drop test
    if self.heightInit == 0:
      self.bkY1 = self.whlY1 + self.sT*(1-self.sSag) 
    else:
      # "+ height" allows user to lift the bike off the ground for a drop test
      # no inital sag for drop test
      self.bkY1 = self.whlY1 + self.sT + self.heightInit       
    
    # reset pre-allocated results data vectors
    self.whlY = np.multiply(self.whlY1, np.ones(self.t.size))
    self.whlvY= np.zeros(self.t.size)
    self.whlaY= np.zeros(self.t.size)
    self.bkX  = np.multiply(self.bkX1,  np.ones(self.t.size))
    self.bkY =  np.multiply(self.bkY1,  np.ones(self.t.size))
    self.bkvX = np.zeros(self.t.size)
    self.bkvY = np.zeros(self.t.size)
    self.bkv = np.zeros(self.t.size)
    self.bkaX = np.zeros(self.t.size)
    self.bkaY = np.zeros(self.t.size)
    self.whlAlpha = np.zeros(self.t.size)
    self.whlW = np.zeros(self.t.size)
    self.inAir = np.zeros(self.t.size)
    self.whlfn = np.zeros(self.t.size)
    self.whlft = np.zeros(self.t.size)
    self.whlfY = np.zeros(self.t.size)
    self.whlfX = np.zeros(self.t.size)
    self.bkfY = np.zeros(self.t.size)
    self.bkfX = np.zeros(self.t.size)
    self.sTY  = np.multiply(self.sT,  np.ones(self.t.size))
    self.sFk = np.zeros(self.t.size)
    self.sFb = np.zeros(self.t.size)
    self.sFt = np.zeros(self.t.size)
    self.bkDragX = np.zeros(self.t.size)
    self.bkDragY = np.zeros(self.t.size)
    self.trkYt = np.zeros(self.t.size)
    self.trkThetat = np.zeros(self.t.size)
    self.throttle = np.zeros(self.t.size)
    
    # time step tracker
    self.i = 0
    
    # episode tracker
    self.done = False
    self.time = 0
    self.reward = 0
    
    state = self.mk_state()
    return state
  
  @property
  def states(self):
    # state is vector concatenated of vectors and scalars
    #   [sampled track elevation points for some distance ahead, e.g. 30 meter ahead, sampled every 1 meter ...
    #   distance between wheel and track surface ...
    #   bike speed ...
    #   suspension travel]
    return self._states #dict(shape=(), type='int') #dict(shape=(self.stateShape,), type='float')
  
  # define physics (i.e. rules) of the environment, and how the agents actions impact the environment
  def step(self,throttle,n):
    for r in range(n):
      # save action
      self.throttle[self.i] = throttle
      # step ahead in time
      
      # break if time has reached end of preallocated results vectors or out of track length
      if (self.t[self.i]+self.dt >= self.t_end[self.trkKey]) or (self.bkX[self.i] >= self.trkX[-1]):
        self.done = True
        self.time = self.t[self.i]
        self.raceTimes[self.trkKey].append(self.time)
        self.raceTimesEp[self.trkKey].append(self.episode_cnt)
        if self.time < self.rewards_bestTimes[self.trkKey]:
          self.rewards_bestTimes[self.trkKey] = self.time
          self.reward = self.time*self.rewards_fasterThanBestTimeFactor
          self.rewards_newBestTimeSet = True
          self.bestTimes[self.trkKey].append(self.time)
          self.bestTimesEp[self.trkKey].append(self.episode_cnt)
        else:
           self.reward = 0 
        if self.drawRace_flg:
          self.draw_race(self)
        return
      
      
    
      
      # compute suspension forces. convention, extension=+ve, tension=-ve
      if self.sKnonLin_flg:
        if self.sTY[self.i] < self.sT*0.3:
          sK = self.sK + (self.sT - self.sTY[self.i])/self.sT*self.sK*5
        else:
          sK = self.sK
      else:
        sK = self.sK
        
      if self.i > 0:
        if self.inAir[self.i]:
          self.sFk[self.i]=0
          self.sFb[self.i]=0
          self.sFt[self.i]=self.whlM*self.g*0 
        else:
          self.sFk[self.i] = self.sPL + (self.sT - self.sTY[self.i])*sK
          self.sFb[self.i] = (self.sTY[self.i-1] - self.sTY[self.i])/self.dt*self.sB # compute
          self.sFb[self.i] = min((self.sFbSat,max(self.sFb[self.i],-self.sFbSat))) # saturate
          self.sFt[self.i]=0
      else:
        self.sFk[self.i] = self.sPL + (self.sT - self.sTY[self.i])*sK
        self.sFb[self.i]=0
        self.sFt[self.i]=0
      
      # find available peak torque
      if self.whlW[self.i] > 0:
        pkTrq = min(self.bkTrq, self.bkPwr/self.whlW[self.i])
      else:
        pkTrq = self.bkTrq
    
      # compute torque command
      if (throttle < 0) and (self.bkv[self.i] < self.brkSpdThr):
        cmdTrq = pkTrq*throttle*(1-(self.brkSpdThr - self.bkv[self.i])/self.brkSpdThr) # apply agent command for braking when speed is below ramp down threshold
      else:
        cmdTrq = pkTrq*throttle # apply agent command for accelerating in forward direction
      
      # compute wheel forces
      # compute track angle at this time instant
      self.trkThetat[self.i] = np.interp(self.bkX[self.i],self.trkX,self.trkTheta)
      if self.inAir[self.i]:
        self.whlfn[self.i]=0
        self.whlft[self.i]=0
        self.whlfX[self.i]=0
        self.whlfY[self.i]=0 # self.whlM*self.g-; this is irrelevant since it could only be used to extend susp when in air, but susp should be extended to leave the groud
      else:
        self.whlfn[self.i]=max(0, (self.whlM*self.g*-1+(self.sFk[self.i] + self.sFb[self.i]))*np.cos(self.trkThetat[self.i])) # normal force cannot have tension, therefore saturate to 0
        self.whlftMax = self.whlfn[self.i]*self.nu
        if cmdTrq >= 0:
          self.whlft[self.i] = min(cmdTrq/self.whlR,  self.whlftMax) + (self.whlM*self.g-(self.sFk[self.i] + self.sFb[self.i]))*np.sin(self.trkThetat[self.i])
        else:
          self.whlft[self.i] = max(cmdTrq/self.whlR,  -self.whlftMax) + (self.whlM*self.g-(self.sFk[self.i] + self.sFb[self.i]))*np.sin(self.trkThetat[self.i])
        self.whlfX[self.i] = -self.whlfn[self.i]*np.sin(self.trkThetat[self.i]) + self.whlft[self.i]*np.cos(self.trkThetat[self.i])
        self.whlfY[self.i] = self.whlfn[self.i]*np.cos(self.trkThetat[self.i]) + self.whlft[self.i]*np.sin(self.trkThetat[self.i])
      
      # compute bike free body in Y-direction and integrate
      # compute Y component drag forces
      self.bkDragY[self.i] = self.bkAero*self.bkvY[self.i]*self.bkvY[self.i]*np.sign(self.bkvY[self.i])*-1
      # HACK: include whlft*sin(theta) to account for vertial acceleration caused by traction on a slope, without going through wheel force balance and suspsension force balance to get the force throuogh from hweel to bike
      self.bkfY[self.i] = (self.bkM*self.g + self.sFk[self.i] + self.sFb[self.i] + self.sFt[self.i] + self.bkDragY[self.i] + self.whlft[self.i]*np.sin(self.trkThetat[self.i]))
      self.bkaY[self.i] = self.bkfY[self.i] / self.bkM 
      self.bkvY[self.i+1]  = self.bkvY[self.i]  + self.bkaY[self.i] * self.dt
      self.bkY[self.i+1]   = self.bkY[self.i]   + self.bkvY[self.i] * self.dt
        
      # compute bike free body in X-direction and integrate
      # compute X component drag forces
      if abs(self.bkvX[self.i]) > 0.01:
        self.bkDragX[self.i] = (self.bkAero*self.bkvX[self.i]*self.bkvX[self.i] - self.bkCrr*-self.g*(self.bkM+self.whlM))*np.sign(self.bkvX[self.i])*-1
      # compute bike acceleration that results from longitudinal force
      if self.inAir[self.i]:
        self.bkfX[self.i] = self.bkDragX[self.i]
        self.bkaX[self.i] = self.bkfX[self.i]/(self.bkM+self.whlM)
      else:
        self.bkfX[self.i] = self.whlfX[self.i] + self.bkDragX[self.i]
        self.bkaX[self.i] = self.bkfX[self.i]/(self.bkM+self.whlM)

      # integrate acceleration for veolcity and position
      # bike longitudinal (bike X and whlX are the same)
      self.bkvX[self.i+1]  = self.bkvX[self.i]  + self.bkaX[self.i] * self.dt
      self.bkX[self.i+1]   = self.bkX[self.i]   + self.bkvX[self.i] * self.dt
      
      # if the bike has gone backwards off the start end of the track, take some actions
      if self.bkX[self.i+1] < self.trkX[0]:
        
        if self.bkvX[self.i+1] < -0.5:
          # if the bike was going fast, end episode and penalize
          self.done = True
          self.time = self.t[self.i]
          self.reward = -1e6
          if self.drawRace_flg:
            self.draw_race(self)
          return
        else:
          #if the bike was going very slow, its because the first throttle cmd was negative, and numerical error, etc.,so just move the bike back to the start position
          self.bkvX[self.i+1]  = 0
          self.bkX[self.i+1]   = 0
      
      
      # bike speed
      self.bkv[self.i+1] = np.sqrt(np.square(self.bkvX[self.i+1]) + np.square(self.bkvY[self.i+1]))
      # wheel rotational speed
      self.whlW[self.i+1] = self.bkv[self.i+1]/self.whlR
      
      # find track elevation at this time step and next time step
      self.trkYt[self.i] = np.interp(self.bkX[self.i],self.trkX,self.trkY)
      self.trkYt2 = np.interp(self.bkX[self.i+1],self.trkX,self.trkY)
      
      # detect whether the bike and wheel will still be in contact with ground next time step
      if (self.bkY[self.i+1] - self.sT - self.whlR) > self.trkYt2:
        self.inAir[self.i+1] = True
      
      # detect if wheel will contact the ground this loop
      if self.inAir[self.i]:
        # compute wheel free body. a = F/m
        self.whlaY[self.i] = (self.whlM*self.g - self.sFk[self.i] - self.sFb[self.i] + self.sFt[self.i]) / self.whlM
        self.whlvY[self.i+1] = self.whlvY[self.i] + self.whlaY[self.i] * self.dt
        self.whlY[self.i+1]  = self.whlY[self.i]  + self.whlvY[self.i] * self.dt
        # detect and assert suspension travel extension limits
        if self.bkY[self.i+1] - self.whlY[self.i+1] > self.sT:
          self.whlY[self.i+1] = self.bkY[self.i+1] - self.sT
        # detect and assert wheel has cotacted the ground
        if (self.whlY[self.i+1]-self.whlR) < self.trkYt2:
          self.whlY[self.i+1] = self.trkYt2 + self.whlR
          self.inAir[self.i+1] = False
      else:
        self.whlY[self.i+1] = self.trkYt2 + self.whlR
          
      # compute suspension travel for next loop
      self.sTY[self.i+1] = self.bkY[self.i+1] - self.whlY[self.i+1]
      
      # increment time index
      self.i+=1
      
    # done for loop
    # if not terminated, set reward to small negative value to encourage finishing race faster
    if self.time < self.rewards_bestTimes[self.trkKey]:
      self.reward = -self.dt*n
    else:
      self.reward = -self.dt*n*self.rewards_slowerThanBestTimeFactor
    
    return

